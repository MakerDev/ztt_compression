# config.yaml
base_llama_1b: &base_llama_1b
  model_name: meta-llama/Llama-3.2-1B-Instruct
  output_dir: /data/save_files/llama-comp-mmlu

  pruned_layers: []
  cycle_layers: []
  cycle_count: 2

  use_lora: false
  lora_rank: 32
  lora_alpha: 16.0
  lora_dropout: 0.1
  train_all: true

  use_distillation: false
  distillation_temperature: 3.0
  distillation_alpha: 0.5
  
  batch_size: 4
  gradient_accumulation_steps: 16
  learning_rate: 2e-4
  num_epochs: 1
  max_seq_length: 1536
  warmup_steps: 50


base_llama_3b: &base_llama_3b
  <<: *base_llama_1b
  model_name: meta-llama/Llama-3.2-3B-Instruct


base_llama_8b: &base_llama_8b
  <<: *base_llama_1b
  model_name: meta-llama/Llama-3.1-8B-Instruct


llama_1b_cycle5:
  <<: *base_llama_1b
  model_name: meta-llama/Llama-3.2-1B-Instruct
  pruned_layers: [10,11,12,13,15]
  cycle_layers: [5,6,7,8,9]
  train_all: true
  use_lora: true
  use_distillation: true
  batch_size: 4
  gradient_accumulation_steps: 16

llama_3b_cycle7:
  <<: *base_llama_3b
  pruned_layers: [20, 21, 22, 23, 24, 25, 26]
  cycle_layers: [13, 14, 15, 16, 17, 18, 19]
  cycle_count: 2
  train_all: true
  use_lora: false
  use_distillation: true
  batch_size: 4
  gradient_accumulation_steps: 16